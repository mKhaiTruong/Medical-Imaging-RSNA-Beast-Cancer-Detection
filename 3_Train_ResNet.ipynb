{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93aab45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: minhkhai0402 (minhkhai0402-i-h-c-vi-t-c) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Color Scheme: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAABlCAYAAABX2Ak/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAABBNJREFUeJzt3b+LHGUAxvF3Z2f3ErlDiSGICLZ2dkEUxdQiaKGNjaU2FnZiYSU2FiKIlWBjo8UVgm1stPAvEKwkGiREUe+4H7uzuzJ7nGLh3exejvFJPp9mArdHXh522C+3e8lgtndjUQAAAlV9HwAAYF1CBgCIJWQAgFhCBgCIJWQAgFhCBgCIJWQAgFh1lwfN5/O//zwYDM7zPAAAZbH455+5q6rqbCHT2tndPfupAABWsLW5eeLXO4dM64Pt7fLDzzdX+ZZ72pPPPFFef+rZ8vE3X5ebO3/0fZwID2/dv9xs+/aP5ddm0vdxYjxYj8uLlx8tP333WZnu3ur7OBFGm1fKI1dfKb98eb1Mf/u97+PEGF16oDz0/LXy/aefl/1bt/s+ToSLVy6Xx159uXz7/kdlx2toZ8ONcbn69punPq5TyBy/nTRpmnIw8eLS1XQ+X27XXiezWd/HidqsXasp/veMrmbH9+m8KYvZtO/jZJg3R5u192bj/uxsNlvutmiaMp94rnXRbtVuNp9Oy+zQa+iqTvtIiw/7AgCxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACxhAwAEEvIAACx6i4PWiwWy+u4rsuF8fi8z3TXGFXVcrv2Oh4O+z5O1GbtWnUZ9H2cGMPj+7Sqy2A46vs4Gar6aLP23qzdn50Nh8vdBnVdqrHnWhftVu1m1WhUhhteQ7s63mr5fBv89+vBYLZ346hSTnA4mZaDg/3OfzkAwJ2wtblZqqo6209kmtmsvPvJV+Wd114o913YuCMHuxfsH07KWx9+Ud5746VyUYV3YrP12G11NluP3VZns/Ucvxt0mk4h05bQn3sHy+tJP97h39qtDqfN8mq3bmy2HrutzmbrsdvqbLaerlv5sC8AEEvIAAB3d8jUw2F57unHl1e6s9vqbLYeu63OZuux2+psdr46/dYSAMD/kbeWAIBYQgYAiCVkAIBYQgYAiCVkAIBYQgYAiCVkAIBYQgYAKKn+ApJF1tglgXskAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 700x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Handle datasets\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import tifffile as tiff\n",
    "import imageio.v3 as iio\n",
    "import SimpleITK as sitk\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "# 2. Visualize datasets\n",
    "import datetime as dtime\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as pff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython.display import display_html\n",
    "\n",
    "# 3. Preprocess datasets\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "## import iterative impute\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "## fastai\n",
    "# from fastai.data.all import *\n",
    "# from fastai.vision.all import *\n",
    "\n",
    "# 4. machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "## for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 5. Deep Learning\n",
    "## Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import ViTModel, ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "## Torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import FloatTensor, LongTensor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.models import resnet34, resnet50, ResNet50_Weights\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 6. metrics\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 7. ignore warnings   \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 8. For displaying and wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# 0. Customization\n",
    "parent_dir = r\"E:\\rsna-breast-cancer-detection\"\n",
    "WANDB_PROJ_NAME = \"RSNA_Breast_Cancer_Detection\"\n",
    "CONFIG = {\n",
    "    'competition': 'RSNA_Breast_Cancer',\n",
    "    '_wandb_kernel': 'aot'\n",
    "}\n",
    "\n",
    "my_colors = ['#517664', '#73AA90', '#94DDBC', '#DAB06C',\n",
    "             '#DF928E', '#C97973', '#B25F57']\n",
    "CMAP1 = ListedColormap(my_colors)\n",
    "print(\"Notebook Color Scheme: \")\n",
    "sns.palplot(sns.color_palette(my_colors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77902980",
   "metadata": {},
   "source": [
    "#### Importing utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f138e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import set_seed, show_values_on_bars\n",
    "from utils import save_dataset_artifact, create_wandb_plot, create_wandb_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecefb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_id', 'patient_id', 'image_id', 'laterality', 'view', 'age',\n",
       "       'cancer', 'biopsy', 'invasive', 'BIRADS', 'implant', 'density',\n",
       "       'machine_id', 'difficult_negative_case'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(parent_dir, \"train.csv\"))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da84b21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC', 'MLO', 'ML', 'LM', 'AT', 'LMO']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['view'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47e267",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a02e1",
   "metadata": {},
   "source": [
    "#### 1. Label Encoding\n",
    "**laterality**:\n",
    "+ 0: L (Left)\n",
    "+ 1: R (Right)\n",
    "\n",
    "**view**:\n",
    "+ 0: AT\n",
    "+ 1: CC \n",
    "+ 2: MLO\n",
    "+ 3: ML\n",
    "+ 4: LM\n",
    "+ 5: LMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee980f",
   "metadata": {},
   "source": [
    "we will try to keep columns that presents in the test + target variable (meaning laterality and view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f09eba",
   "metadata": {},
   "source": [
    "#### Normal PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a966d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95744c4a98b4fbe9cfaf9256c780d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54706 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image_path = os.path.join(parent_dir, \"train_image_processed_cv2_512\")\n",
    "\n",
    "all_paths = []\n",
    "for k in tqdm(range(len(df))):\n",
    "    row = df.iloc[k, :]\n",
    "    all_paths.append(\n",
    "        os.path.join(\n",
    "            train_image_path, str(row.patient_id), f\"{str(row.image_id)}.png\"\n",
    "        ) \n",
    "    )\n",
    "    \n",
    "df['path'] = all_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317975c6",
   "metadata": {},
   "source": [
    "### JP2000 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054e3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image_path = os.path.join(parent_dir, \"train_image_ROI_processed_jp2000_512\")\n",
    "\n",
    "# all_paths = []\n",
    "# for k in tqdm(range(len(df))):\n",
    "#     row = df.iloc[k, :]\n",
    "#     all_paths.append(\n",
    "#         os.path.join(\n",
    "#             train_image_path, str(row.patient_id), f\"{str(row.image_id)}.jp2\"\n",
    "#         ) \n",
    "#     )\n",
    "    \n",
    "# df['path'] = all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d16651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site_id',\n",
       " 'patient_id',\n",
       " 'image_id',\n",
       " 'laterality',\n",
       " 'view',\n",
       " 'age',\n",
       " 'implant',\n",
       " 'machine_id',\n",
       " 'prediction_id']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(parent_dir, \"test.csv\"))\n",
    "test_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8873ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>laterality</th>\n",
       "      <th>view</th>\n",
       "      <th>age</th>\n",
       "      <th>implant</th>\n",
       "      <th>cancer</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10006</td>\n",
       "      <td>462822612</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\rsna-breast-cancer-detection\\train_image_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10006</td>\n",
       "      <td>1459541791</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\rsna-breast-cancer-detection\\train_image_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10006</td>\n",
       "      <td>1864590858</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\rsna-breast-cancer-detection\\train_image_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10006</td>\n",
       "      <td>1874946579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\rsna-breast-cancer-detection\\train_image_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011</td>\n",
       "      <td>220375232</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E:\\rsna-breast-cancer-detection\\train_image_pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id    image_id  laterality  view   age  implant  cancer  \\\n",
       "0       10006   462822612           0     1  61.0        0       0   \n",
       "1       10006  1459541791           0     5  61.0        0       0   \n",
       "2       10006  1864590858           1     5  61.0        0       0   \n",
       "3       10006  1874946579           1     1  61.0        0       0   \n",
       "4       10011   220375232           0     1  55.0        0       0   \n",
       "\n",
       "                                                path  \n",
       "0  E:\\rsna-breast-cancer-detection\\train_image_pr...  \n",
       "1  E:\\rsna-breast-cancer-detection\\train_image_pr...  \n",
       "2  E:\\rsna-breast-cancer-detection\\train_image_pr...  \n",
       "3  E:\\rsna-breast-cancer-detection\\train_image_pr...  \n",
       "4  E:\\rsna-breast-cancer-detection\\train_image_pr...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[\n",
    "    ['patient_id', 'image_id', 'laterality', 'view', 'age', 'implant',\n",
    "     \"cancer\", \"path\"]\n",
    "]\n",
    "\n",
    "le_laterality = LabelEncoder()\n",
    "le_view = LabelEncoder()\n",
    "\n",
    "df['laterality'] = le_laterality.fit_transform(df['laterality'])\n",
    "df['view']       = le_view.fit_transform(df['view'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6a7cd",
   "metadata": {},
   "source": [
    "#### 2. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a05631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id     0\n",
       "image_id       0\n",
       "laterality     0\n",
       "view           0\n",
       "age           37\n",
       "implant        0\n",
       "cancer         0\n",
       "path           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499eaf7",
   "metadata": {},
   "source": [
    "'age' has 37 missing values. We can impute it with the mean and then normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b72cf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    54669.000000\n",
       "mean        58.543928\n",
       "std         10.050884\n",
       "min         26.000000\n",
       "25%         51.000000\n",
       "50%         59.000000\n",
       "75%         66.000000\n",
       "max         89.000000\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f182e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].fillna(58)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9367",
   "metadata": {},
   "source": [
    "This can be fast and simple, and also is a good technique when the missing values are just a few. However if the feature is skewed (missing values are not random), other approaches can prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6331fd",
   "metadata": {},
   "source": [
    "Here is the rule:\n",
    "\n",
    "% Missing-------Action\n",
    "\n",
    "< 1%------------Fill it, don't sweat it\n",
    "\n",
    "1–5%------------Fill with median or imputer\n",
    "\n",
    "\\> 10%-----------Be cautious – maybe impute smartly or explore why it’s missing\n",
    "\n",
    "\\> 30%-----------Reconsider using the column at all unless it's critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113aea22",
   "metadata": {},
   "source": [
    "#### 3. Save Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afef52dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\MINH KHAI\\Healthcare_Imaging\\RSNA_BreastCancer\\wandb\\run-20250419_164009-amu9g1d1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection/runs/amu9g1d1' target=\"_blank\">save_train_prep</a></strong> to <a href='https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection' target=\"_blank\">https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection/runs/amu9g1d1' target=\"_blank\">https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection/runs/amu9g1d1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">save_train_prep</strong> at: <a href='https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection/runs/amu9g1d1' target=\"_blank\">https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection/runs/amu9g1d1</a><br> View project at: <a href='https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection' target=\"_blank\">https://wandb.ai/minhkhai0402-i-h-c-vi-t-c/RSNA_Breast_Cancer_Detection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250419_164009-amu9g1d1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact has been save successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(os.path.join(parent_dir, \"train_preprocessed.csv\"), index=False)\n",
    "\n",
    "save_dataset_artifact(run_name = \"save_train_prep\",\n",
    "                      artifact_name=\"train_prep\",\n",
    "                      path = os.path.join(parent_dir, \"train_preprocessed.csv\"),\n",
    "                      projectName = WANDB_PROJ_NAME,\n",
    "                      config = CONFIG,\n",
    "                      data_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1dd386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['patient_id', 'image_id', 'laterality', 'view', 'age', 'implant',\n",
       "       'cancer', 'path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed\n",
    "set_seed()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available now: ', DEVICE)\n",
    "\n",
    "# Read in Data\n",
    "train = pd.read_csv(os.path.join(parent_dir, \"train_preprocessed.csv\"))\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9f01a",
   "metadata": {},
   "source": [
    "#### 4. Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83961acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== GLOBAL PARAMS =======\n",
    "csv_columns = [\"laterality\", \"view\", \"age\", \"implant\"]\n",
    "no_columns = len(csv_columns)\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019db1b",
   "metadata": {},
   "source": [
    "## Training Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f037cf9",
   "metadata": {},
   "source": [
    "### 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1120f",
   "metadata": {},
   "source": [
    "### If use ROI-extracted files then no need for random crop\n",
    "        # A.RandomResizedCrop(\n",
    "        # size=(224, 224), \n",
    "        # scale=(0.08, 1.0), \n",
    "        # ratio=(0.75, 1.333), \n",
    "        # p=1.0\n",
    "        # ),\n",
    "        # A.ShiftScaleRotate(rotate_limit=9.0, scale_limit=[0.8, 1.2], p=1.0),\n",
    "\n",
    "### == Resizing ==\n",
    "        # A.Resize(224, 224), # Pretrained-model (ResNet, EfficientNet) < 224x224\n",
    "        # But resizing can be bad since medical imaging may keen on original size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07820c",
   "metadata": {},
   "source": [
    "### === ALTERNATIVE TO RESIZING: Isotropic scaling ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d2c810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import transforms    # augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43b2feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transforms(isTrain = False):\n",
    "#     aug_list = []\n",
    "    \n",
    "#     if isTrain:\n",
    "#         aug_list += [\n",
    "            \n",
    "#             # === Spatial: Isotropic Scaling ===\n",
    "#             A.LongestMaxSize(max_size=224),\n",
    "#             A.PadIfNeeded(min_height=224, min_width=224, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            \n",
    "#             # === Photometric Augmentations - Brightness / Contrast / Gamma — very mild ===\n",
    "#             A.OneOf([\n",
    "#                 A.RandomToneCurve(scale=0.3, p=0.5),\n",
    "#                 A.RandomGamma(gamma_limit=(90, 110), p=0.3),\n",
    "#                 A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.2), contrast_limit=(-0.4, 0.5), p=0.5)\n",
    "#             ], p=0.5),\n",
    "            \n",
    "#             # === Mild Contrast Enhancer (safe CLAHE) ===\n",
    "#             A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.2),\n",
    "            \n",
    "#             # == Downscaling - Blurring (slight only) ==\n",
    "#             A.OneOf([\n",
    "#                 A.MotionBlur(blur_limit=3, p=0.3),\n",
    "#                 A.Downscale(scale_min=0.9, scale_max=0.95, interpolation=dict(\n",
    "#                     upscale=cv2.INTER_LINEAR, downscale=cv2.INTER_AREA), p=0.7),\n",
    "#             ], p=0.2),\n",
    "            \n",
    "#             # === Occlusion-style Augmentation ===\n",
    "#             A.OneOf([\n",
    "#                 A.GridDropout(ratio=0.2, unit_size_min=16, unit_size_max=32, random_offset=True, p=0.2),\n",
    "#                 A.CoarseDropout(max_holes=6, max_height=0.15, max_width=0.25, min_holes=1, min_height=0.05, min_width=0.1, fill_value=0, mask_fill_value=None, p=0.25),\n",
    "#             ], p=0.3),\n",
    "            \n",
    "#             # === Flips ===\n",
    "#             A.HorizontalFlip(p=0.5) if isTrain else A.NoOp(),\n",
    "#             A.VerticalFlip(p=0.5) if isTrain else A.NoOp(),\n",
    "            \n",
    "#         ]\n",
    "    \n",
    "#     else:\n",
    "#         # Inference-time: keep only resize + normalize\n",
    "#         aug_list += [\n",
    "#             A.LongestMaxSize(max_size=224),\n",
    "#             A.PadIfNeeded(min_height=224, min_width=224, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "#         ]\n",
    "    \n",
    "#     # === Normalize & ToTensor ===\n",
    "#     aug_list += [\n",
    "#         A.Normalize(),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "    \n",
    "#     return A.Compose(aug_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e29a11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataFrame, isTrain = True, transforms=None):\n",
    "        self.dataFrame, self.isTrain = dataFrame, isTrain\n",
    "        self.transforms = transforms    ## Data Augmentation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataFrame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            row = self.dataFrame.iloc[index]\n",
    "            csv_data = np.array(row[csv_columns].values, dtype=np.float32)\n",
    "            image_path = self.dataFrame['path'][index]\n",
    "\n",
    "            if not os.path.exists(image_path):  # Check if image exists\n",
    "                return self.__getitem__((index + 1) % len(self))\n",
    "            \n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = np.array(image)\n",
    "            if image is None:\n",
    "                return self.__getitem__((index + 1) % len(self))\n",
    "            \n",
    "            \n",
    "            transform_image = self.transforms(image=image)['image']\n",
    "            \n",
    "            if self.isTrain:\n",
    "                return {\n",
    "                    \"image\": transform_image, \n",
    "                    \"meta\": csv_data, \n",
    "                    \"target\": self.dataFrame['cancer'][index]\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"image\": transform_image, \n",
    "                    \"meta\": csv_data, \n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed at index {index}: {e}\")\n",
    "            if self.isTrain:\n",
    "                return {\n",
    "                    \"image\": torch.zeros(3, 224, 224), \n",
    "                    \"meta\": torch.zeros(self.no_columns),\n",
    "                    \"target\": torch.tensor(0)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"image\": torch.zeros(3, 224, 224), \n",
    "                    \"meta\": torch.zeros(self.no_columns), \n",
    "                }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e978aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_device(data):\n",
    "    image, metadata, targets = data.values()\n",
    "    return image.to(DEVICE), metadata.to(DEVICE), targets.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df0abc",
   "metadata": {},
   "source": [
    "### 2. Sanity Check\n",
    "to catch dumb mistakes early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a2129ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \n",
      " Image: torch.Size([3, 3, 224, 224]) \n",
      " Meta: tensor([[ 0.,  1., 61.,  0.],\n",
      "        [ 0.,  5., 61.,  0.],\n",
      "        [ 1.,  5., 61.,  0.]], device='cuda:0') \n",
      " Targets: tensor([0, 0, 0], device='cuda:0')\n",
      "==================================================\n",
      "Batch: 1 \n",
      " Image: torch.Size([3, 3, 224, 224]) \n",
      " Meta: tensor([[ 1.,  1., 61.,  0.],\n",
      "        [ 0.,  1., 55.,  0.],\n",
      "        [ 0.,  5., 55.,  0.]], device='cuda:0') \n",
      " Targets: tensor([0, 0, 0], device='cuda:0')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sample_df = train.head(6)\n",
    "dataset = RSNADataset(\n",
    "    dataFrame = sample_df, \n",
    "    isTrain = True, \n",
    "    transforms=transforms(isTrain=True)\n",
    ")\n",
    "dataLoader = DataLoader(dataset, batch_size=3, shuffle=False, pin_memory=True)\n",
    "\n",
    "for i, data in enumerate(dataLoader):\n",
    "    image, meta, targets = data_to_device(data)\n",
    "    print(f\"Batch: {i} \\n Image: {image.shape} \\n Meta: {meta} \\n Targets: {targets}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b573a26",
   "metadata": {},
   "source": [
    "### 3. ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01c41df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ResNet50Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e84f6",
   "metadata": {},
   "source": [
    "##### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd6175dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape: torch.Size([3, 3, 224, 224]) \n",
      " Input Metadata Shape: torch.Size([3, 4])\n",
      "Features image shape: torch.Size([3, 2048])\n",
      "Metadata shape: torch.Size([3, 500])\n",
      "Concatenated data: torch.Size([3, 2548])\n",
      "Out shape: torch.Size([3, 1])\n",
      "==================================================\n",
      "Loss = 0.6406197547912598\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_example = ResNet50Network(\n",
    "    outSize=output_size,\n",
    "    no_columns=no_columns\n",
    ").to(DEVICE)\n",
    "\n",
    "out = model_example(image, meta, prints=True)\n",
    "\n",
    "criterion_example = nn.BCEWithLogitsLoss()\n",
    "loss = criterion_example(out, targets.unsqueeze(1).float())\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aff94d",
   "metadata": {},
   "source": [
    "### 4. EffNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3e650",
   "metadata": {},
   "source": [
    "**Problem with CNNs as a whole**: They train on fixed resource cost and scaled up when more resource are available. The scaling up - increase arbitrary the depth or width of the CNN, then manually tune -> This is not optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca7537",
   "metadata": {},
   "source": [
    "EffNet uses a compound coefficient in a more structured manner.\n",
    "\n",
    "+ uniformly scale each dimension (w, h, resolution) with a fixed set of scaling coefficients.\n",
    "\n",
    "+ This is much more efficient than scaling only 1 dimension at the time.\n",
    "\n",
    "EffNet also uses AutoML:\n",
    "\n",
    "+ Automated Machine Learning\n",
    "\n",
    "+ Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8eeecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EffNetNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e9ea8",
   "metadata": {},
   "source": [
    "#### sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa857201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n",
      "Input Image Shape: torch.Size([3, 3, 224, 224]) \n",
      " Input Metadata Shape: torch.Size([3, 4])\n",
      "Features image shape: torch.Size([3, 1536])\n",
      "Metadata shape: torch.Size([3, 250])\n",
      "Concatenated data: torch.Size([3, 1786])\n",
      "Out shape: torch.Size([3, 1])\n",
      "==================================================\n",
      "Loss = 0.8133776187896729\n"
     ]
    }
   ],
   "source": [
    "model_example_2 = EffNetNetwork(outputSize=output_size,\n",
    "                                no_columns=no_columns).to(DEVICE)\n",
    "\n",
    "out = model_example_2(image, meta, prints=True)\n",
    "\n",
    "criterion_example = nn.BCEWithLogitsLoss()\n",
    "loss = criterion_example(out, targets.unsqueeze(1).float())\n",
    "print(\"=\"*50)\n",
    "print(f\"Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cf13f",
   "metadata": {},
   "source": [
    "## Vision Transformer - ViT\n",
    "In medical imaging, *Vision Transformers (ViTs)* have demonstrated *superior* performance over EfficientNet models in various diagnostic tasks. For instance, in differentiating benign from malignant lesions in PET/CT scans, ViTs achieved an AUC of *0.90*, outperforming EfficientNet's 0.87 . Similarly, a hybrid model *combining EfficientNetV2 with a ViT* achieved an accuracy of 98.10% in breast cancer detection, surpassing the individual performance of EfficientNetV2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfe571",
   "metadata": {},
   "source": [
    "### 1. Define The Patch Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fad91c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchEmbedding(nn.Module):\n",
    "#     def __init__(self, imgSize, patchSize, inChannels, embeddedDim):\n",
    "#         super(PatchEmbedding, self).__init__()\n",
    "#         self.imgSize, self.imgSize = imgSize, patchSize\n",
    "#         self.inChannels, self.embeddedDim = inChannels, embeddedDim\n",
    "#         self.proj = nn.Conv2d(inChannels, embeddedDim, kernel_size=patchSize, stride=patchSize)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.proj(x)\n",
    "#         x = x.flatten(2)\n",
    "#         x = x.transpose(1, 2)  # Shape: (B, num_patches, embed_dim)\n",
    "#         return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24dfdc8",
   "metadata": {},
   "source": [
    "### 2. Implement the transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b62e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, embeddedDim, num_heads, num_layers):\n",
    "#         super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "#         encoder_layers = nn.TransformerEncoderLayer(\n",
    "#             d_model=embeddedDim,\n",
    "#             nhead=num_heads,\n",
    "#             dim_feedforward=2048\n",
    "#         )\n",
    "#         self.encoder = nn.TransformerEncoder(\n",
    "#             encoder_layer=encoder_layers,\n",
    "#             num_layers=num_layers\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504971a",
   "metadata": {},
   "source": [
    "### 3. Construct the ViT Model\n",
    "Patch embedding + transformer encoder + classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1971dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VisionTransformer(nn.Module):\n",
    "    \n",
    "#     def __init__(self, imgSize, patchSize, inChannels, embeddedDim, num_heads, num_layers, num_classes):\n",
    "#         super(VisionTransformer, self).__init__()\n",
    "        \n",
    "#         self.patchEmbedded = PatchEmbedding(imgSize, patchSize, inChannels, embeddedDim)\n",
    "#         self.encoder = TransformerEncoder(embeddedDim, num_heads, num_layers)\n",
    "#         self.fc = nn.Linear(embeddedDim, num_classes)\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embeddedDim))\n",
    "#         self.pos_embed = nn.parameter(torch.zeros(1, (imgSize / patchSize) ** 2 + 1, embeddedDim))\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.patchEmbedded(x)\n",
    "#         cls_token = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        \n",
    "#         x = torch.cat((cls_token, x), dim=1)\n",
    "#         x = x + self.pos_embed\n",
    "#         x = self.encoder(x)\n",
    "        \n",
    "#         cls_outputs = x[:, 0]\n",
    "#         logits = self.fc(cls_outputs)\n",
    "#         return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e546675",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Or use a pre trained ViT from gg\n",
    "from utils import ViTNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66f498",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed3e7a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image Shape: torch.Size([3, 3, 224, 224]) \n",
      " Input Metadata Shape: torch.Size([3, 4])\n",
      "ViT output shape: torch.Size([3, 768])\n",
      "Metadata output shape: torch.Size([3, 250])\n",
      "Concatenated shape: torch.Size([3, 1018])\n",
      "Final output shape: torch.Size([3, 1])\n",
      "==================================================\n",
      "Loss = 0.5617946982383728\n"
     ]
    }
   ],
   "source": [
    "model_example_3 = ViTNetwork(outSize=output_size,\n",
    "                                no_columns=no_columns).to(DEVICE)\n",
    "\n",
    "out = model_example_3(image, meta, prints=True)\n",
    "\n",
    "criterion_example = nn.BCEWithLogitsLoss()\n",
    "loss = criterion_example(out, targets.unsqueeze(1).float())\n",
    "print(\"=\"*50)\n",
    "print(f\"Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05a561",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9207f91",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dabba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 3\n",
    "EPOCHS = 20\n",
    "MIN_THRESHOLD_EPOCHS = 5\n",
    "PATIENCE = 3\n",
    "WORKERS = 8\n",
    "LR = 0.0005\n",
    "WD = 0.0\n",
    "LR_PATIENCE = 1\n",
    "LR_FACTOR = 0.4\n",
    "\n",
    "BATCH_SIZE1 = 64\n",
    "BATCH_SIZE2 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b43e5",
   "metadata": {},
   "source": [
    "#### Small function to save in an external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "863ba60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1786"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1536 + 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "582d1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_in_file(text, f, trial):\n",
    "    with open(f\"logs_{VERSION}_trial_{trial}.txt\", \"a+\") as f:\n",
    "        print(text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c58553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(model, train_original, lr, batch_size, trial, epochs=3):\n",
    "    \n",
    "    topModels = []\n",
    "    \n",
    "    def saveBestModel(foldNum, epochNum, valid_acc, f1Score, best_roc, model = model, VERSION = VERSION):\n",
    "        model_name = f\"{VERSION}_Fold{foldNum}_Epoch{epochNum}_ValidAcc{valid_acc:.3f}_F1{f1Score:.3f}_ROC{best_roc:.3f}.pth\"\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "        \n",
    "        topModels.append((model_name, valid_roc))\n",
    "        topModels = sorted(topModels, key= lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if len(topModels) > 3:\n",
    "            removedModel = topModels.pop()  # Removing the worst ROC based model\n",
    "            os.remove(removedModel[0])\n",
    "            print(f\"Deleted worst model: {removedModel[0]}\")\n",
    "            \n",
    "    \n",
    "    f = open(f\"logs_{VERSION}.txt\", \"w+\")\n",
    "    print(f\"Training with learning rate: {lr} and batch size: {batch_size}\")\n",
    "    \n",
    "    # Generate indices to split data into training and test set\n",
    "    group_fold = GroupKFold(n_splits=FOLDS)\n",
    "    k_folds = group_fold.split(X=np.zeros(len(train_original)),\n",
    "                               y=train_original['cancer'],\n",
    "                               groups=train_original['patient_id'].tolist())\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(k_folds):\n",
    "        print(f\"-------- Fold: {i+1} --------\")\n",
    "        add_in_file(f\"-------- Fold: {i+1} --------\", f, trial=trial)\n",
    "        \n",
    "        # == WANDB Tracking == \n",
    "        RUN_CONFIG = CONFIG.copy()\n",
    "        params = dict(\n",
    "            model=MODEL,\n",
    "            version=VERSION,\n",
    "            fold=i,\n",
    "            epochs=epochs,\n",
    "            batch=batch_size,\n",
    "            lr=lr,\n",
    "            weight_decay=WD\n",
    "        )\n",
    "        RUN_CONFIG.update(params)\n",
    "        run = wandb.init(project=WANDB_PROJ_NAME, config=RUN_CONFIG)\n",
    "        wandb.watch(model, log_freq=100)\n",
    "        \n",
    "        # == Create Instances ==\n",
    "        best_roc = None\n",
    "        patience_f = PATIENCE\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=WD)\n",
    "        scheduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"max\", patience=LR_PATIENCE, factor=LR_FACTOR)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # == Read in data ==\n",
    "        train_data = train_original.iloc[train_index].reset_index(drop=True)\n",
    "        valid_data = train_original.iloc[valid_index].reset_index(drop=True)\n",
    "        \n",
    "        # == Create Data Instances ==\n",
    "        train = RSNADataset(train_data, isTrain=True, transforms=transforms(isTrain=True))\n",
    "        valid = RSNADataset(valid_data, isTrain=True, transforms=transforms(isTrain=True))\n",
    "        \n",
    "        # == Data Loaders ==\n",
    "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "        valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        # == Epochs ==\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            correct = 0\n",
    "            train_losses = 0\n",
    "            \n",
    "            # == TRAIN ==\n",
    "            model.train()\n",
    "            for k, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)):\n",
    "                image, meta, targets = data_to_device(data)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                out = model(image, meta)\n",
    "                loss = criterion(out, targets.unsqueeze(1).float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Save information\n",
    "                train_losses += loss.item()\n",
    "                wandb.log({'train_loss': loss.item()})\n",
    "                \n",
    "                train_preds = torch.round(torch.sigmoid(out))\n",
    "                correct += (train_preds.cpu() == targets.cpu().unsqueeze(1)).sum().item()\n",
    "                \n",
    "            # Compute train accuracy\n",
    "            train_acc = correct / len(train_index)\n",
    "            wandb.log({\"train_acc\": train_acc})\n",
    "            \n",
    "            # === EVALUATION ===\n",
    "            model.eval()\n",
    "            valid_preds = torch.zeros(size=(len(valid_index), 1), device=DEVICE, dtype=torch.float32)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for k, data in enumerate(tqdm(valid_loader, desc=f\"Epoch {epoch+1} [Valid]\", leave=False)):\n",
    "                    image, meta, targets = data_to_device(data)\n",
    "                    out = model(image, meta)\n",
    "                    preds = torch.sigmoid(out)\n",
    "                    valid_preds[k * image.shape[0]: (k+1) * image.shape[0]] = preds\n",
    "                    \n",
    "                validPredBins = torch.round(valid_preds.cpu())\n",
    "                valid_acc = accuracy_score(valid_data['cancer'].values, validPredBins)\n",
    "                valid_roc = roc_auc_score(valid_data['cancer'].values, valid_preds.cpu())\n",
    "                validF1Score = f1_score(valid_data['cancer'].values, validPredBins)\n",
    "                \n",
    "                wandb.log({\"valid_acc\": valid_acc})\n",
    "                wandb.log({\"valid_roc\": valid_roc})\n",
    "                wandb.log({\"valid_F1Score\": validF1Score})\n",
    "                \n",
    "                # Print info\n",
    "                duration = str(time.time() - start_time)[:7]\n",
    "                final_logs = '{} | Epoch: {}/{} | Loss: {:.4} | Acc_tr: {:.3} | Acc_vd: {:.3} | ROC: {:.3} | F1 Score: {:.3}'.format(\n",
    "                    duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc, validF1Score)\n",
    "                add_in_file(final_logs, f, trial=trial)\n",
    "                print(final_logs)\n",
    "                \n",
    "                #=== SAVE MODEL===\n",
    "                \n",
    "                # Update scheduler (for learning_rate)\n",
    "                scheduler.step(valid_roc)\n",
    "                \n",
    "                # Update best Roc\n",
    "                # If best_roc = None\n",
    "                if best_roc is None: \n",
    "                    best_roc = valid_roc\n",
    "                    saveBestModel(foldNum=i+1, epochNum=epoch+1, valid_acc=valid_acc, f1Score=validF1Score, best_roc=best_roc, model = model, VERSION = VERSION)\n",
    "                    print(f\"Saved initial best model at Epoch {epoch+1}\")\n",
    "                    continue\n",
    "            \n",
    "                if valid_roc > best_roc:\n",
    "                    best_roc = valid_roc\n",
    "                    saveBestModel(foldNum=i+1, epochNum=epoch+1, valid_acc=valid_acc, f1Score=validF1Score, best_roc=best_roc, model = model, VERSION = VERSION)\n",
    "                    \n",
    "                    # Reset patience (because we have improvement)\n",
    "                    patience_f = PATIENCE\n",
    "                    \n",
    "                else:\n",
    "                    # Decrease patience (no improvement in ROC)\n",
    "                    patience_f = patience_f - 1\n",
    "                    \n",
    "                    if epoch + 1 > MIN_THRESHOLD_EPOCHS and patience_f == 0:\n",
    "                        stop_logs = 'Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n",
    "                                    format(best_roc)\n",
    "                        add_in_file(stop_logs, f, trial=trial)\n",
    "                        print(stop_logs)\n",
    "                        break\n",
    "        \n",
    "        # === CLEANING ===\n",
    "        # Clear memory\n",
    "        del train, valid, train_loader, valid_loader, image, targets\n",
    "        gc.collect()\n",
    "        \n",
    "        # Finalize Wandb\n",
    "        wandb.finish()\n",
    "\n",
    "    return best_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50dc04",
   "metadata": {},
   "source": [
    "### Sanity check for training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1cba8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the sanity check with a small subset of data...\n",
      "Dataset instances created for training and validation.\n",
      "Model, optimizer, and criterion initialized.\n",
      "Epoch 1 started. Training Loop...\n",
      "Training completed. Train Loss: 0.7512 | Train Acc: 0.500\n",
      "Validating batch 1...\n",
      "Epoch 1 completed. Duration: 0.25107s | Valid Acc: 0.000 | Valid ROC: nan\n",
      "Sanity check completed.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "FOLDS = 3\n",
    "EPOCHS = 20\n",
    "MIN_THRESHOLD_EPOCHS = 5\n",
    "PATIENCE = 3\n",
    "WORKERS = 8\n",
    "LR = 0.0005\n",
    "WD = 0.0\n",
    "LR_PATIENCE = 1\n",
    "LR_FACTOR = 0.4\n",
    "\n",
    "BATCH_SIZE1 = 64\n",
    "BATCH_SIZE2 = 32\n",
    "\n",
    "# Setup logging configuration\n",
    "log_file = \"sanity_check_log.txt\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, \n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def log_and_print(message):\n",
    "    \"\"\"Logs and prints messages to track code execution.\"\"\"\n",
    "    print(message)\n",
    "    logging.info(message)\n",
    "\n",
    "# Select a small subset for one fold (e.g., the first 2 samples)\n",
    "sample_train_data = train.head(2)\n",
    "sample_valid_data = train.head(2)\n",
    "\n",
    "log_and_print(\"Starting the sanity check with a small subset of data...\")\n",
    "\n",
    "# Create Dataset instances for training and validation (with simplified batch size)\n",
    "train_dataset = RSNADataset(sample_train_data, isTrain=True, transforms=transforms(isTrain=True))\n",
    "valid_dataset = RSNADataset(sample_valid_data, isTrain=True, transforms=transforms(isTrain=True))\n",
    "\n",
    "log_and_print(\"Dataset instances created for training and validation.\")\n",
    "\n",
    "# Function to create DataLoader\n",
    "def create_dataloader(dataset):\n",
    "    \"\"\"Create a new DataLoader instance.\"\"\"\n",
    "    return DataLoader(dataset, \n",
    "                      batch_size=3, \n",
    "                      shuffle=False, \n",
    "                      pin_memory=True)\n",
    "\n",
    "# Initialize DataLoader and iterate over it\n",
    "train_loader = create_dataloader(train_dataset)\n",
    "valid_loader = create_dataloader(valid_dataset)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "# Initialize Model and Optimizer\n",
    "model_example = ResNet50Network(outSize=output_size, no_columns=no_columns).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model_example.parameters(), lr=LR, weight_decay=WD)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "log_and_print(\"Model, optimizer, and criterion initialized.\")\n",
    "\n",
    "# Train for one epoch to check if everything works\n",
    "for epoch in range(1):  # Run only one epoch for the test\n",
    "    start_time = time.time()\n",
    "    log_and_print(f\"Epoch {epoch+1} started. Training Loop...\")\n",
    "\n",
    "    # === Training Loop ===\n",
    "    model_example.train()\n",
    "    correct_train = 0\n",
    "    train_loss = 0\n",
    "    step = 0  # Keep track of how many batches we've processed\n",
    "\n",
    "    while step < len(train_loader):  # Continue until we process all the data\n",
    "        try:\n",
    "            data = next(train_loader_iter)  # Try fetching the next batch\n",
    "            images, meta, targets = data_to_device(data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model_example(images, meta)\n",
    "\n",
    "            loss = criterion(out, targets.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            preds = torch.round(torch.sigmoid(out))\n",
    "            correct_train += (preds.cpu() == targets.cpu().unsqueeze(1)).sum().item()\n",
    "\n",
    "            step += 1\n",
    "        except Exception as e:\n",
    "            # If a worker fails, recreate the DataLoader and restart iteration\n",
    "            log_and_print(f\"Worker failed at step {step}. Re-creating DataLoader: {e}\")\n",
    "            train_loader = create_dataloader(train_dataset)  # Pass the dataset argument here\n",
    "            train_loader_iter = iter(train_loader)  # Restart iteration\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_acc = correct_train / len(sample_train_data)\n",
    "    log_and_print(f\"Training completed. Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "\n",
    "    # === Validation Loop ===\n",
    "    model_example.eval()\n",
    "    correct_valid = 0\n",
    "    valid_preds = torch.zeros(len(sample_valid_data), 1, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            if i % 10 == 0:  # Every 10 batches, print a progress update\n",
    "                log_and_print(f\"Validating batch {i+1}...\")\n",
    "\n",
    "            images, meta, targets = data_to_device(data)\n",
    "\n",
    "            out = model_example(images, meta)\n",
    "            preds = torch.sigmoid(out)\n",
    "\n",
    "            valid_preds[i * images.shape[0] : (i+1) * images.shape[0]] = preds\n",
    "\n",
    "            correct_valid += (torch.round(preds) == targets.unsqueeze(1)).sum().item()\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    valid_acc = correct_valid / len(sample_valid_data)\n",
    "    \n",
    "    # Calculate ROC for validation\n",
    "    valid_roc = roc_auc_score(sample_valid_data['cancer'].values, valid_preds.cpu())\n",
    "\n",
    "    # Calculate epoch duration\n",
    "    duration = str(time.time() - start_time)[:7]\n",
    "\n",
    "    log_and_print(f\"Epoch {epoch+1} completed. Duration: {duration}s | \"\n",
    "                  f\"Valid Acc: {valid_acc:.3f} | Valid ROC: {valid_roc:.3f}\")\n",
    "\n",
    "    # Save model after each epoch if needed\n",
    "    model_name = f\"test_epoch{epoch+1}_train_acc{train_acc:.3f}_valid_acc{valid_acc:.3f}_roc{valid_roc:.3f}.pth\"\n",
    "    # torch.save(model_example.state_dict(), model_name)\n",
    "\n",
    "    # Clear memory after each epoch\n",
    "    del train_loader, valid_loader, images, targets, out\n",
    "    gc.collect()\n",
    "\n",
    "log_and_print(\"Sanity check completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6d86e",
   "metadata": {},
   "source": [
    "## Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21cbfd",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ade0d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION = 'v1'\n",
    "# MODEL = 'resnet50'\n",
    "\n",
    "# model1 = ResNet50Network(outSize=output_size, no_columns=no_columns).to(DEVICE)\n",
    "\n",
    "# #---------------------------------------------------------------------------------------\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to tune\n",
    "#     trial_number = trial.number\n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    \n",
    "#     # Initialize the model\n",
    "#     model = ResNet50Network(outSize=output_size, no_columns=no_columns).to(DEVICE)\n",
    "    \n",
    "#     # Train and validate the model\n",
    "#     best_roc = train_folds(\n",
    "#         model, \n",
    "#         train_original=train, \n",
    "#         lr=lr, \n",
    "#         batch_size=batch_size, \n",
    "#         trial = trial_number,\n",
    "#         epochs=EPOCHS\n",
    "#     )\n",
    "    \n",
    "#     return best_roc\n",
    "\n",
    "# #---------------------------------------------------------------------------------------\n",
    "# study = optuna.create_study(direction='maximize')  # Maximize ROC AUC\n",
    "# study.optimize(objective, n_trials=2)\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# f = open(\"logs_v1.txt\", \"r\")\n",
    "# contents = f.read()\n",
    "# print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c6ad7",
   "metadata": {},
   "source": [
    "### Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df3adfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION = 'v3'\n",
    "# MODEL = 'effnet50'\n",
    "\n",
    "# #---------------------------------------------------------------------------------------\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to tune\n",
    "#     trial_number = trial.number\n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    \n",
    "#     # Initialize the model\n",
    "#     model = EffNetNetwork(outputSize=output_size, no_columns=no_columns).to(DEVICE)\n",
    "    \n",
    "#     # Train and validate the model\n",
    "#     best_roc = train_folds(\n",
    "#         model, \n",
    "#         train_original=train, \n",
    "#         lr=lr, \n",
    "#         batch_size=batch_size, \n",
    "#         trial = trial_number,\n",
    "#         epochs=EPOCHS\n",
    "#     )\n",
    "    \n",
    "#     return best_roc\n",
    "\n",
    "# #---------------------------------------------------------------------------------------\n",
    "# study = optuna.create_study(direction='maximize')  # Maximize ROC AUC\n",
    "# study.optimize(objective, n_trials=2)\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# f = open(\"logs_v1.txt\", \"r\")\n",
    "# contents = f.read()\n",
    "# print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192644cb",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d17fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION = 'v4'\n",
    "# MODEL = 'ViTNetwork'\n",
    "\n",
    "# #---------------------------------------------------------------------------------------\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to tune\n",
    "#     trial_number = trial.number\n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [128])\n",
    "    \n",
    "#     # Initialize the model\n",
    "#     model = ViTNetwork(outSize=output_size, no_columns=no_columns).to(DEVICE)\n",
    "    \n",
    "#     # Train and validate the model\n",
    "#     best_roc = train_folds(\n",
    "#         model, \n",
    "#         train_original=train, \n",
    "#         lr=lr, \n",
    "#         batch_size=batch_size, \n",
    "#         trial = trial_number,\n",
    "#         epochs=EPOCHS\n",
    "#     )\n",
    "    \n",
    "#     return best_roc\n",
    "\n",
    "# #---------------------------------------------------------------------------------------\n",
    "# study = optuna.create_study(direction='maximize')  # Maximize ROC AUC\n",
    "# study.optimize(objective, n_trials=2)        \n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# f = open(\"logs_v1.txt\", \"r\")\n",
    "# contents = f.read()\n",
    "# print(contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
